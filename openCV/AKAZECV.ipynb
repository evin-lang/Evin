{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# In[14]:\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\"\"\"\n",
    "#メモ\n",
    "①黒板(対象物)をとる\n",
    "②授業動画をとる\n",
    "③①と②の画像一枚一枚を見比べる(特徴の判定）\n",
    "④特徴から画像を切り出す\n",
    "⑤切り出した画像を透視返還(台形返還）\n",
    "⑥ノートにする\n",
    "​\n",
    "普通に特徴点をとったほうがいいかも\n",
    "しいて言えば二値化すれば...\n",
    "↑\n",
    "今のところcannyと二値化しか試してないので他のedge検知のアルゴリズムを試してみるべき？\n",
    "あとはフーリエ変換かな。。。。\n",
    "正面の傾きには強いが横からなど角度がつくと極端にダメになる\n",
    "機械学習を用いた対応が必要になるのではないか？？？？\n",
    "2019/05/11記入\n",
    "\"\"\"\n",
    "std = \"img/output/\"\n",
    "stdin = \"img/input/\"\n",
    "testcase1 =[\"IMG_78645\",\"IMG_7864\",\"IMG_7865\",\"IMG_7866\",\"IMG_7867\",\"IMG_7868\",\"IMG_7869\",\"IMG_7870\",\"IMG_7871\",\n",
    "            \"IMG_7872\",\"IMG_7873\",\"IMG_7874\",\"IMG_7875\"] \n",
    "now = testcase1\n",
    "TARGET = 0\n",
    "IMAGE = []\n",
    "img,image,gray,rgba=[],[],[],[]\n",
    "timg,tgray,trgba=[],[],[]\n",
    "def setup(string):\n",
    "    string+=\".JPG\"\n",
    "    global img\n",
    "    global IMAGE\n",
    "    img.append(cv2.imread(stdin+string))\n",
    "    IMAGE.append(copy.deepcopy(img))\n",
    "    global gray \n",
    "    gray.append(cv2.imread(stdin+string,0))\n",
    "    global rgba \n",
    "    rgba.append(cv2.imread(stdin+string,-1))\n",
    "def setuptmp(string):\n",
    "    string+=\".png\"\n",
    "    global timg \n",
    "    timg.append(cv2.imread(stdin+string))\n",
    "    global tgray \n",
    "    tgray.append(cv2.imread(stdin+string,0))\n",
    "    global trgba \n",
    "    trgba.append(cv2.imread(stdin+string,-1))\n",
    "for i in range(0,len(now)):\n",
    "    setup(now[i])\n",
    "    #cv2.imwrite(stdin+now[i]+\".png\",img[i])\n",
    "    now[i]+=\".jpg\"\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴点検出AkAZE法with 2値化\n",
    "#目標画像の加工\n",
    "# 閾値\n",
    "t = 200\n",
    "# 方法2値化\n",
    "ret,th = [],[]\n",
    "for i in range(0,len(now)):\n",
    "    ref,tmp = cv2.threshold(gray[i],t,255,CV_THRESH_TOZERO_INV)\n",
    "    tmp = cv2.bitwise_not(tmp)\n",
    "    ref,tmp = cv2.threshold(tmp, 0, 255, cv2.THRESH_BINARY| CV_THRESH_OTSU);\n",
    "    ret.append(ref) \n",
    "    th.append(tmp)\n",
    "    # 結果を出力\n",
    "    cv2.imwrite(std+\"th2\"+now[i], tmp)\n",
    "\n",
    "akaze = cv2.AKAZE_create()\n",
    "#特徴点比較AKAZE法with 2値化\n",
    "kp, des = [],[]\n",
    "for i in th:\n",
    "    ref,tmp= akaze.detectAndCompute(i, None)\n",
    "    kp.append(ref)\n",
    "    des.append(tmp)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "for i in range(1,len(now)):\n",
    "    matches = bf.match(des[TARGET], des[i])\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    img_compare = cv2.drawMatches(img[TARGET], kp[TARGET], img[i], kp[i], matches[:10], None, flags=2)\n",
    "    cv2.imwrite(std+\"thcompare\"+now[TARGET]+\"&\"+now[i],img_compare)\n",
    "\n",
    "bf = cv2.BFMatcher()\n",
    "for i in range(1,len(now)):\n",
    "    matches = bf.knnMatch(des[TARGET], des[i], k=2)\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.5*n.distance:\n",
    "            good.append([m])\n",
    "    img_compare2 = cv2.drawMatchesKnn(img[TARGET], kp[TARGET], img[i], kp[i], good, None, flags=2)\n",
    "    cv2.imwrite(std+\"thcompare2\"+now[TARGET]+\"&\"+now[i],img_compare2)\n",
    "\n",
    "def alignImages(img1, img2,i,\n",
    "                max_pts=500, good_match_rate=0.15, min_match=10):\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html#feature-homography\n",
    "\n",
    "    # [1] ORBを用いて特徴量を検出する\n",
    "    # Initiate ORB detector\n",
    "    detector = cv2.ORB_create(max_pts)\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = detector.detectAndCompute(\n",
    "        cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), None\n",
    "    )\n",
    "    kp2, des2 = detector.detectAndCompute(\n",
    "        cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY), None\n",
    "    )\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    # [2] 検出した特徴量の比較をしてマッチングをする\n",
    "    # Match descriptors.\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    good = matches[:int(len(matches) * good_match_rate)]\n",
    "\n",
    "    # [3] 十分な特徴量が集まったらそれを使って入力画像を変形する\n",
    "    if len(good) > min_match:\n",
    "        src_pts = np.float32(\n",
    "            [kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32(\n",
    "            [kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        # Find homography\n",
    "        h, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC)\n",
    "\n",
    "        cv2.imwrite(std+\"changed\"+now[i], \n",
    "                    cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=2))\n",
    "\n",
    "        # Use homography\n",
    "        height, width, channels = img1.shape\n",
    "        dst_img = cv2.warpPerspective(img2, h, (width, height))\n",
    "        return dst_img\n",
    "    else:\n",
    "        return img1, np.zeros((3, 3))\n",
    "\n",
    "for i in range(1,len(now)): \n",
    "    change = alignImages(img[TARGET],img[i],i)\n",
    "    #第一引数に目標画像、第二引数に対象画像\n",
    "    cv2.imwrite(std+\"changed\"+now[i],change)\n",
    "print(\"fin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\features2d\\src\\akaze.cpp:172: error: (-215:Assertion failed) ! image.empty() in function 'cv::AKAZE_Impl::detectAndCompute'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-81d6d14fe1b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mkp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0makaze\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mkp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mdes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\features2d\\src\\akaze.cpp:172: error: (-215:Assertion failed) ! image.empty() in function 'cv::AKAZE_Impl::detectAndCompute'\n"
     ]
    }
   ],
   "source": [
    "#特徴点検出AkAZE法with 2値化\n",
    "#目標画像の加工\n",
    "# 閾値\n",
    "t = 200\n",
    "# 方法2値化\n",
    "ret,th = [],[]\n",
    "for i in range(0,len(now)):\n",
    "    ref,tmp = cv2.threshold(gray[i],t,255,cv2.THRESH_TOZERO_INV)\n",
    "    tmp = cv2.bitwise_not(tmp)\n",
    "    ref,tmp = cv2.threshold(tmp, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU);\n",
    "    ret.append(ref) \n",
    "    th.append(tmp)\n",
    "    # 結果を出力\n",
    "    cv2.imwrite(std+\"th2\"+now[i], tmp)\n",
    "\n",
    "akaze = cv2.AKAZE_create()\n",
    "#特徴点比較AKAZE法with 2値化\n",
    "kp, des = [],[]\n",
    "for i in th:\n",
    "    ref,tmp= akaze.detectAndCompute(i, None)\n",
    "    kp.append(ref)\n",
    "    des.append(tmp)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "for i in range(1,len(now)):\n",
    "    matches = bf.match(des[TARGET], des[i])\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    img_compare = cv2.drawMatches(img[TARGET], kp[TARGET], img[i], kp[i], matches[:10], None, flags=2)\n",
    "    cv2.imwrite(std+\"thcompare\"+now[TARGET]+\"&\"+now[i],img_compare)\n",
    "\n",
    "bf = cv2.BFMatcher()\n",
    "for i in range(1,len(now)):\n",
    "    matches = bf.knnMatch(des[TARGET], des[i], k=2)\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.5*n.distance:\n",
    "            good.append([m])\n",
    "    img_compare2 = cv2.drawMatchesKnn(img[TARGET], kp[TARGET], img[i], kp[i], good, None, flags=2)\n",
    "    cv2.imwrite(std+\"thcompare2\"+now[TARGET]+\"&\"+now[i],img_compare2)\n",
    "\n",
    "def alignImages(img1, img2,i,\n",
    "                max_pts=500, good_match_rate=0.15, min_match=10):\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html#feature-homography\n",
    "\n",
    "    # [1] ORBを用いて特徴量を検出する\n",
    "    # Initiate ORB detector\n",
    "    detector = cv2.ORB_create(max_pts)\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = detector.detectAndCompute(\n",
    "        cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), None\n",
    "    )\n",
    "    kp2, des2 = detector.detectAndCompute(\n",
    "        cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY), None\n",
    "    )\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    # [2] 検出した特徴量の比較をしてマッチングをする\n",
    "    # Match descriptors.\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    good = matches[:int(len(matches) * good_match_rate)]\n",
    "\n",
    "    # [3] 十分な特徴量が集まったらそれを使って入力画像を変形する\n",
    "    if len(good) > min_match:\n",
    "        src_pts = np.float32(\n",
    "            [kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32(\n",
    "            [kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        # Find homography\n",
    "        h, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC)\n",
    "\n",
    "        cv2.imwrite(std+\"changed\"+now[i], \n",
    "                    cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=2))\n",
    "\n",
    "        # Use homography\n",
    "        height, width, channels = img1.shape\n",
    "        dst_img = cv2.warpPerspective(img2, h, (width, height))\n",
    "        return dst_img\n",
    "    else:\n",
    "        return img1, np.zeros((3, 3))\n",
    "\n",
    "for i in range(1,len(now)): \n",
    "    change = alignImages(img[TARGET],img[i],i)\n",
    "    #第一引数に目標画像、第二引数に対象画像\n",
    "    cv2.imwrite(std+\"changed\"+now[i],change)\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-5-1e01df84d58b>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-1e01df84d58b>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    ​\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "#特徴点検出AkAZE法\n",
    "akaze = cv2.AKAZE_create()\n",
    "#特徴点比較AKAZE法\n",
    "kp, des = [],[]\n",
    "for i in gray:\n",
    "    ref,tmp= akaze.detectAndCompute(i, None)\n",
    "    kp.append(ref)\n",
    "    des.append(tmp)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "for i in range(1,len(now)):\n",
    "    matches = bf.match(des[TARGET], des[i])\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    img_compare = cv2.drawMatches(img[TARGET], kp[TARGET], img[i], kp[i], matches[:10], None, flags=2)\n",
    "    cv2.imwrite(std+\"compare\"+now[0]+\"&\"+now[i],img_compare)\n",
    "\n",
    "bf = cv2.BFMatcher()\n",
    "for i in range(1,len(now)):\n",
    "    matches = bf.knnMatch(des[TARGET], des[i], k=2)\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.5*n.distance:\n",
    "            good.append([m])\n",
    "    img_compare2 = cv2.drawMatchesKnn(img[TARGET], kp[TARGET], img[i], kp[i], good, None, flags=2)\n",
    "    cv2.imwrite(std+\"compare2\"+now[TARGET]+\"&\"+now[i],img_compare2)\n",
    "\n",
    "def alignImages(img1, img2,i,\n",
    "                max_pts=500, good_match_rate=0.15, min_match=10):\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html#feature-homography\n",
    "\n",
    "    # [1] ORBを用いて特徴量を検出する\n",
    "    # Initiate ORB detector\n",
    "    detector = cv2.ORB_create(max_pts)\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = detector.detectAndCompute(\n",
    "        cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), None\n",
    "    )\n",
    "    kp2, des2 = detector.detectAndCompute(\n",
    "        cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY), None\n",
    "    )\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    # [2] 検出した特徴量の比較をしてマッチングをする\n",
    "    # Match descriptors.\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    good = matches[:int(len(matches) * good_match_rate)]\n",
    "\n",
    "    # [3] 十分な特徴量が集まったらそれを使って入力画像を変形する\n",
    "    if len(good) > min_match:\n",
    "        src_pts = np.float32(\n",
    "            [kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32(\n",
    "            [kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        # Find homography\n",
    "        h, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC)\n",
    "\n",
    "        cv2.imwrite(std+\"gchanged\"+now[i], \n",
    "                    cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=2))\n",
    "\n",
    "        # Use homography\n",
    "        height, width, channels = img1.shape\n",
    "        dst_img = cv2.warpPerspective(img2, h, (width, height))\n",
    "        return dst_img\n",
    "    else:\n",
    "        return img1, np.zeros((3, 3))\n",
    "\n",
    "for i in range(1,len(now)): \n",
    "    change = alignImages(img[TARGET],img[i],i)\n",
    "    #第一引数に目標画像、第二引数に対象画像\n",
    "    cv2.imwrite(std+\"gchanged\"+now[i],change)\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
